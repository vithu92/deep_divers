{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_url_list(path) -> list:\n",
    "    with open(path, \"r\") as json_file:\n",
    "        url_list = json.load(json_file)['url']\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrapper(url:str=None) -> BeautifulSoup:\n",
    "    # Chrome Settings\n",
    "    chrome_option = Options()\n",
    "    chrome_option.add_argument(\"--headless\") #background task; don't open a window\n",
    "    chrome_option.headless = True\n",
    "    chrome_option.add_argument('--disable-gpu')\n",
    "    chrome_option.add_argument('--no-sandbox')#I copied this, so IDK?\n",
    "    chrome_option.add_argument('--disable-dev-shm-usage')#this too\n",
    "    chrome_option.add_argument('--disable-extensions')\n",
    "    # Initatlize Chrome Service and the webdriver\n",
    "    chrome_service = Service(ChromeDriverManager(print_first_line=False, log_level=logging.NOTSET).install())\n",
    "    chrome_service.SuppressInitialDiagnosticInformation = True\n",
    "    driver = webdriver.Chrome(service=chrome_service, options=chrome_option)\n",
    "    # Scrap html body\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    # Convert to a beautiful soup object    \n",
    "    return BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(url:str=None) -> pd.DataFrame:\n",
    "    soup:BeautifulSoup = web_scrapper(url=url)\n",
    "    json_body = json.loads(soup.find_all(\"meta\", attrs={\"data-pagecontent-json\" : True})[0][\"data-pagecontent-json\"])\n",
    "    for index in range(len(json_body['anchors'])):\n",
    "        if json_body['anchors'][index]['name'] == \"productTile-new\":\n",
    "            product_data = json_body['anchors'][index]['json']['elements']\n",
    "    \n",
    "    if not product_data:\n",
    "        raise(\"No product data found\")\n",
    "    df = pd.read_json(StringIO(json.dumps(product_data)))\n",
    "    if 'saving' not in df.columns:\n",
    "        df['saving'] = np.nan\n",
    "    if 'savingText' not in df.columns:\n",
    "        df['savingText'] = np.nan\n",
    "    \n",
    "    selected_columns = [\"title\",\"href\",\"quantity\",\"ratingValue\",\"price\",\"saving\",\"savingText\"]\n",
    "    \n",
    "    df= df[selected_columns]\n",
    "\n",
    "    # renaming \n",
    "    if 'saving' in df.columns:\n",
    "        df.rename(columns = {'saving':'oldPrice'}, inplace = True)\n",
    "    if 'ratingValue' in df.columns:\n",
    "        df.rename(columns= {'ratingValue':'rating'}, inplace=True)\n",
    "\n",
    "    df['category_1'] = df['href'].apply(lambda row: row.split(\"/\")[3])\n",
    "    df['category_2'] = df['href'].apply(lambda row: row.split(\"/\")[4])\n",
    "    df['category_3'] = df['href'].apply(lambda row: row.split(\"/\")[5])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(df:pd.DataFrame) -> None:\n",
    "    today = datetime.now()\n",
    "    month = today.strftime(\"%m\")\n",
    "    day = today.strftime(\"%d\")\n",
    "    \n",
    "    export_directory = f\"./exports/{today.year}_{month}_{day}\"\n",
    "    isExists = os.path.exists(export_directory)\n",
    "    if not isExists:\n",
    "        os.makedirs(export_directory)\n",
    "    filename = f\"{export_directory}/{today.year}_{month}_{day}_{df.iloc[0]['category_2']}.csv\"\n",
    "    print(filename)\n",
    "    df.to_csv(path_or_buf=filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------1 of 51------------------------------\n"
     ]
    }
   ],
   "source": [
    "url_list:list = load_url_list(\"./coop_url.json\")\n",
    "df_master = None\n",
    "counter = 1\n",
    "for url in url_list:\n",
    "    print(\"---\"*10 + f\"{counter} of {len(url_list)}\" + \"---\"*10)\n",
    "    counter +=1\n",
    "    try:    \n",
    "        df = extract_data(url=url)\n",
    "        export_csv(df=df)\n",
    "    except Exception as e:\n",
    "        print(\"Error while scrapping\")\n",
    "        print(f\"Error message: {e}\")\n",
    "        break\n",
    "    try:\n",
    "        if not isinstance(df_master,pd.DataFrame):\n",
    "            df_master = df\n",
    "        else:\n",
    "            df_master.append(df, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"could not merge dataframe for the url {url}\")\n",
    "        print(e)\n",
    "        break       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "\n",
    "url = \"https://www.coop.ch/en/food/dairy-products-eggs/milk/c/m_0056?q=%3AtopRated&sort=name-asc&pageSize=300&page=1\"\n",
    "#url = \"https://www.coop.ch/en/food/fruit-vegetables/fruit/c/m_0002?page=1&pageSize=100&q=%3Aname-asc&sort=name-asc\"\n",
    "soup:BeautifulSoup = web_scrapper(url=url)\n",
    "json_body = json.loads(soup.find_all(\"meta\", attrs={\"data-pagecontent-json\" : True})[0][\"data-pagecontent-json\"])\n",
    "\n",
    "for index in range(len(json_body['anchors'])):\n",
    "    if json_body['anchors'][index]['name'] == \"productTile-new\":\n",
    "        product_data = json_body['anchors'][index]['json']['elements']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
